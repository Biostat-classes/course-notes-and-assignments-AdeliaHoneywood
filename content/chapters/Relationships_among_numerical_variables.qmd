---
title: Relationships among numerical variables
subtitle: When we only have numbers
bibliography: ../references.bib
---

<!-- COMMENT NOT SHOW IN ANY OUTPUT: Code chunk below sets overall defaults for .qmd file; these inlcude showing output by default and looking for files relative to .Rpoj file, not .qmd file, which makes putting filesin different folders easier  -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

In the last chapter we extended linear models to consider impacts of
multiple factors. Continuing that tradition, we will now explore how
numerical (and specifically continuous) predictor variables can be used
to explain variation in numerical outcome variables.

## Back to the iris data

We will motivate this with an example from our favorite iris data. So
far we have considered how species impacts measured outcomes. However,
we might also want to consider the relationship between traits. For
example, we might want to know if sepal and petal length are related. We
could plot the data:

```{r}
library(ggplot2)
sepal_petal_relationship <- ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +
  geom_point() +
  labs(title="Sepal length increases with petal length",
       x= "Sepal length (cm)",
       y= "Petal Length (cm)")
sepal_petal_relationship

```

Our related hypothesis might be a relationship exists among the
variables; alternatively, one does not. To put this our null hypothesis
framework, we might write:

$$
\begin{split}
H_O: \textrm{there is not a relationship between sepal and petal length}\\
H_A: \textrm{there is a relationship between sepal and petal length}\\
\end{split}
$$

In other words, we need to gather enough evidence to reject the
hypothesis of no relationship. Note we will formalize these hypotheses
more in a moment, but how do we test them?

### Working with continuous predictors

For some background, consider differences between continuous and
categorical predictor variables. Unlike examples of when we transformed
continuous outcomes into binomial data, continuous predictors offers
information on order and spacing. Compared to un-ordered categorical
variables (what we've focused on), the numbers mean something! This
allows us (with caution) to estimate outcomes from un-sampled regions.

Consider: We know the mean petal lengths for three species of irises:

```{r}
library(Rmisc)
function_output <- summarySE(iris, measurevar="Sepal.Length", groupvars =
                               c("Species"))

ggplot(function_output, aes(y=Sepal.Length, x=Species, fill=Species)) +
  geom_col(aes(fill=Species)) +
    geom_errorbar(aes(ymin=Sepal.Length-ci, ymax=Sepal.Length+ci)) +
  labs(title=expression(paste("Sepal lengths of ",italic("I. species"))),
       y= "Sepal length (cm)",
       x= "Species")
```

But if we find a new species, we actually don't know what to expect!

However, if we have this graph

```{r}
sepal_petal_relationship
```

We might have a guess of the petal length of a flower that has a sepal
length of 2.5 cm even though we didn't measure anything of that size. In
fact, you might be able to visualize the relationship:

```{r}
sepal_petal_relationship+geom_smooth(method = "lm",se = F)
```

While we can draw this "line of best fit" by sight, linear models
actually give us a way to formally define it (analyze our hypothesis).
The line of best fit minimizes the residuals (which means it also offers
a prediction any value of sepal length!). This approach, however, also
means we are considering **linear** relationships between our variables.
This is one assumption of linear regression! We can consider
**non-linear** relationships using other techniques (which we will cover
later).

If we wanted to carry out a sampling experiment to determine a p-value
for our hypotheses, we could sample from a population that represents
our sepal lengths and one that represents our petal lengths. If the two
populations are not connected, then arbitrary pairs could be made - this
would indicate no relationship among the variables. For each dataset, we
could note the potential relationship (now occuring by chance!) between
our variables, and then compare that null distribution to what we
actually observed. To make this approach more generalizable, we could
standardize our data points - using these to calculate our error terms
for full and reduced models would lead us to F distributions.

Overall, this means we can use a linear model approach to investigate
relationships among numerical variables. We can build the model

```{r}
iris_regression <- lm(Sepal.Length ~ Petal.Length, iris)
```

and see the impacts on our linear model.

The relationship between our two variables is noted in the model matrix,
which now includes our actual values,

```{r}
library(rmarkdown)
paged_table(data.frame(model.matrix(iris_regression)))
```

and a coefficient that shows their relationship. The first parameter is
the x-intercept, and the second (the relationship) is the slope of the
best fit line!

```{r}
matrix(as.numeric(iris_regression$coefficients), ncol=1)
```

Once the model is made, we can plot it to check assumptions

```{r}
plot(iris_regression)

```

and if assumptions are met consider outcomes.

```{r}
library(car)
Anova(iris_regression, type = "III")
summary(iris_regression)

```

Here the ANOVA table tells us there is a significant impact of sepal
length on petal length, and the summary (and graph) demonstrate that
relationship is positive. Note we do not need any post-hoc analysis (why
not?).

## Regression or correlation

This approach, with small changes in theory, can be used for two
scenarios. We typically consider (but actually rarely use) linear
regression. Linear regression technically assumes that an approach was
used to determine how one variable impacts another (so we chose which
one to vary and how). For this scenario, our sampling experiment
technically uses the set values of the predictor variable,and our
hypotheses focus on the coefficient value

$$
\begin{split}
H_O: \beta_\textrm{relationship between sepal and petal length} = 0\\
H_A: \beta_\textrm{relationship between sepal and petal length} \neq 0\\
\end{split}
$$

Note our coefficient will also change based on measurement units, but
this should not impact the resulting p-value.

The other approach, correlation, is more simply measuring association
between the variables. It's not specifying which, if either, is the
driver - both variables could be responding to an un-measured variable.
For example, since we simply observed flower traits, we could easily
reverse everything above (plot shown here).

```{r}
ggplot(iris, aes(y=Petal.Length, x=Sepal.Length)) +
  geom_point() +
  labs(title="Petal length increases with sepal length",
       x= "Sepal length (cm)",
       y= "Petal Length (cm)")
```

Note that doing so would change the coefficients in our linear model,but
not the direction of the relationship. For this reason, correlation
often focuses on a unitless correlation parameter, *r*, instead of a
coefficient from our $\beta$ matrix.

$$
\begin{split}
H_O: r_\textrm{relationship between sepal and petal length} = 0\\
H_A: r_\textrm{relationship between sepal and petal length} \neq 0\\
\end{split}
$$

*r* can vary from -1 (values are perfectly negatively related) to 1
(values are perfectly positively related), where 0 indicates no
associatoin. This should sound familiar (hold this thought). For a
related sampling experiment, populations for both traits are simulated
and then paired (what we described above!).

The correlation coefficient can be calculated in R using the *cor.test*
function; note the formula interface is different to reflect
association.

```{r}
cor.test(~ Sepal.Length + Petal.Length, data = iris)
```

Note

## Plotting outcomes

## Next steps

In the next chapters we will carry our linear model approach to consider
the relationship between continuous outcomes and continuous predictor
variables.
