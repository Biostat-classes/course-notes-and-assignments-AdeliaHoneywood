---
title: Linear model extensions
subtitle: How to fix it when it's broken
bibliography: ../references.bib
---

<!-- COMMENT NOT SHOW IN ANY OUTPUT: Code chunk below sets overall defaults for .qmd file; these inlcude showing output by default and looking for files relative to .Rpoj file, not .qmd file, which makes putting filesin different folders easier  -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

The past several chapters/lessons have focused on linear models. That's
because they are a great way to unify most/many tests from classical
statistics. In fact, most of the ranked tests we've developed can
actually be run as linear models when n \>15. For example, we can go
back to our Wilcox-Mann Whitney U tests (for 2 populations) and
Kruskal-Wallis (for 3+) from the [comparing means among groups
chapter](Compare_means_among_populations.qmd){target="_blank"} and note
the outcome from a *wilcox.test*

```{r}
two_species_subset <- iris[iris$Species!="setosa",]
wilcox.test(Sepal.Length ~ Species, two_species_subset)
```

is very close to a linear model predicting the signed rank of the data

```{r}
library(car)
signed_rank = function(x) sign(x) * rank(abs(x))
Anova(lm(signed_rank(Sepal.Length) ~ Species, two_species_subset), type="III")
```

In fact, we could run simulations and show that p values from these 2
approaches are highly correlated (now you know what that means) with a
$\beta$ of almost 1 (from @lindelov).

```{r, echo=F}
library(tidyverse)
set.seed(21)
weird_data <- c(rnorm(10000), exp(rnorm(10000)), runif(10000, min=-3, max=-2))


# Parameters
Ns = c(seq(from=6, to=20, by=2), 30, 50, 80)
mus = c(0, 0.5, 1)  # Means
PERMUTATIONS = 1:200

# Run it
D = expand.grid(set=PERMUTATIONS, mu=mus, N=Ns) %>%
  mutate(
    # Generate data. One normal and one weird
    data = map2(N, mu, ~cbind(sample(weird_data, .x), .y + rnorm(.x))),
    
    # Built-in
    mann_raw = map(data, ~ wilcox.test(.x[,1], .x[,2])),
    
    # Ttest
    ranked_value = map(data, ~ rank(c(.x))),  # As 1D ranked vector for t.test
    ttest_raw = map2(ranked_value, N, ~t.test(.x[1:.y], .x[-(1:.y)], var.equal=TRUE)),
    
    # Tidy it up
    mann = map(mann_raw, broom::tidy),
    ttest = map(ttest_raw, broom::tidy)
  ) %>%
  
  # Get as columns instead of lists; then remove "old" columns
  unnest(mann, ttest, .sep='_') %>%
  select(-data, -mann_raw, -ranked_value, -ttest_raw)
D$N = factor(D$N)  # Make N a factor for prettier plotting

library(ggplot2)
library(patchwork)

# A straight-up comparison of the p-values
library(ggpubr)

p_relative = ggplot(D, aes(x=mann_p.value, y=ttest_p.value, color=N)) + 
  geom_line() + 
  geom_vline(xintercept=0.05, lty=2) +
  geom_hline(yintercept=0.05, lty=2) +
  
  labs(title='Absolute relation', x = 'Mann-Whitney p-value', y = 'T-test p-value') + 
  #coord_cartesian(xlim=c(0, 0.10), ylim=c(0, 0.11)) + 
  theme_gray(13) + 
  guides(color=FALSE)+
    stat_cor(p.accuracy = 0.001, r.accuracy = 0.01)

# Looking at the difference (error) between p-values
p_error_all = ggplot(D, aes(x=mann_p.value, y=ttest_p.value-mann_p.value, color=N)) + 
  geom_line() + 
  geom_vline(xintercept=0.05, lty=2) +
  labs(title='Error', x = 'Mann-Whitney p-value', y = 'T-test p-value deviation') + 
  theme_gray(13) + 
  guides(color=FALSE)
# Same, but zoomed in around p=0.05
p_error_zoom = ggplot(D, aes(x=mann_p.value, y=ttest_p.value-mann_p.value, color=N)) + 
  geom_line() + 
  geom_vline(xintercept=0.05, lty=2) +
  
  labs(title='Zoomed error', x = 'Mann-Whitney p-value', y = 'T-test p-value deviation') + 
  coord_cartesian(xlim=c(0, 0.10), ylim=c(-0.020, 0.000)) + 
  theme_gray(13)

# Show it. Patchwork is your friend!
p_relative + p_error_all + p_error_zoom
```

Linear models are also extremely robust. Consider the basic assumptions
of a linear model

$$
\epsilon \approx i.i.d.\ N(\mu,\sigma)
$$

Although the residuals are meant to be homoscedastic (equal or constant
across all groups) , it turns out the model is robust of when the
largest group variance is 4-10x larger than the smallest group variance
and sample sizes are approximately equal [@blanca2018; @fox2015;
@zuur2010], though highly uneven group sizes begin to cause issues
[@blanca2018].

Similarly, non-normal data is not an issue. This is partially because
the assumptions are focused on residuals, but also because the procedure
is highly robust [@blanca2017]. This finding further supports the
graphical consideration of assumptions, especially since many tests for
normality are conservative (samples are almost never *perfectly* normal,
and slight deviations are easier to pick up with larger sample sizes
despite the fact the central limit theorem suggests this is when the
issue is least important @zuur2010).

These issues have led some authors to argue violations of the linear
model assumptions are less dangerous than trying new, and often
less-tested, techniques [@knief2021]. Alternatively, new techniques (or
sometimes old techniques that are new to a given field) may be more
appropriate when assumptions are clearly broken [@warton2010;
@reitan2016; @geissinger2022]. In this section we explore common-ish
approaches for analyzing data when the assumptions of a linear model are
broken. Fortunately most can be viewed as extensions to our existing
knowledge.

## Residuals are not appropriately distributed (heteroscedastic or not normal)

Although this may be viewed as 2 separate violations, many datasets will
violate both simultaneously. Addressing this issue is often best
resolved by understanding *why* this is happening.

### Linear relationship is inappropriate

### Data are not independent

#### In respect to predictors

A major issue for linear models is when predictors are co-linear.
Mathematically speaking, perfect collinearity occurs when any column of
the design (*X*) matrix can be derived by combining other columns.
Perfect collinearity will lead to a message noting singularity issues,
which is R's way of telling you the matrix isn't invertible (which it
has to be to solve the equation).

Even partial collinearity will lead to an increase in Type II errors
[@zuur2010]. To put it simply, partitioning variance among related
predictors is hard. For this reason, a few approaches may be used.

##### Check for issues

The first step is identifying issues. From the outset, relationships
among predictor variables can be assessed using the *pairs* function in
R. If two variables are highly correlated (r^2^ \> .8 is a general
limit), only one should be included in the model. Similarly, variance
inflation factors (vif) can be assessed for the final and other models
to consider this issues (all this is covered in the [previous chapter
that introduces multiple
regression](Combining_numerical_and_categorical_predictors.qmd){target="_blank"}.

#### In respect to measured outcomes

When outcome variables are linked together, a few options exist. Note
this issue may be obvious from checks of assumptions, but it also may be
due to experimental design.

Consider this example. In order to investigate impacts of climate stress
on oysters, specimens are placed in individual tanks and held at normal
summer (calculated using recent data) temperature or at temperatures
predicted under 2 IPCC --- Intergovernmental Panel on Climate Change-
scenarios. Oysters were also exposed to predator cues by dropping in
water from tanks with 0, low (.25/m2), or high (2/m2) predators. After
two months changes in oyster shell length (growth) was measured.
Twenty-five oysters were measured for each treatment combination.

You hopefully recognize this as a factorial ANOVA experiment that you
know how to analyze (see the [chapter on ANOVA
extensions](%22More_ANOVAs.qmd%22){target="_blank"}). Experiments like
this are odd, however, given the space they require. It is far more
common to put lots of organisms in a single container given space and
costs. However, this means our measurements are connected (remember
[blocking and paired tests](%22More_ANOVAs.qmd%22){target="_blank"})?

There are several ways to deal with this. Here we explore each for our
oyster example.

```{r, echo=F}
set.seed(19)
experiment <- data.frame(temperature = factor(c(rep("ambient",75), rep("elevated_scenario1", 75), rep("elevated_scenario2", 75))),
                         predator_cue = factor(c(rep(c(rep("none",25),
                                               rep("normal",25),
                                               rep("high",25)),3))),
                         growth = c(rnorm(25,2),
                                    rnorm(25, 1.5),
                                    rnorm(25, .7),
                                    rnorm(25,2.7),
                                    rnorm(25,1.0),
                                    rnorm(25, .4),
                                    rnorm(25, 3.1),
                                    rnorm(25, 3.1),
                                    rnorm(25, 3.1)),
                         container = c(rep(letters[1:9],each=25)))
experiment[experiment$growth<0, "growth"] <- 0
```

##### Ignore it (don't do this!)

First, let's ignore the lack of independence. This is *not* an option,
but it let's you see the impact.

```{r}
growth_lm <- lm(growth~predator_cue*temperature, experiment)
plot(growth_lm)
library(car)
Anova(growth_lm, type="III")
```

We find significant main effects and interactions using this *wrong*
approach.

##### Find average for each unit

One way of doing this focuses on the average for each unit

```{r}
library(Rmisc)
averaged_experiment <- summarySE(experiment, measurevar = "growth",
                             groupvars = c("predator_cue", "temperature", "container"))
library(rmarkdown)
paged_table(averaged_oyster)
```

and use that for your analysis.

```{r, error=T}
average_analysis <- lm(growth~predator_cue*temperature, averaged_experiment)
Anova(average_analysis, type = "III")
```

but that leads to an issue! Since we only get 9 average outcomes and our
model requires 10 degrees of freedom (consider why), we are left with no
"noise" to make the denominator for our F ratio! Even when this doesn't
happen, you have reduced your data to a much smaller number of points
and are not getting credit for all your work! This is a good example of
why you should analyze simulated data before you run an experiment, but
there are other options.

#####Blocking

The blocking approach we've already covered works might seem appropriate
here.

```{r, error=T}
blocking_analysis <- lm(growth~predator_cue*temperature+container, experiment)
Anova(blocking_analysis, type = "III")
```

but its not? Why? This error means now our model matrix has collinearity
issues. WE can actually see where

```{r}
alias(blocking_analysis)
```

though the output is confusing. In general, the issue here is each unit
only contributes to one level of other traits..so if we know the average
impact of ambient temperatures, for example, and the impacts in two of
the treatments that were held at that temperature, we can predict the
other. If instead each unit contributed to multiple levels ([think back
to our feather experiment](%22More_ANOVAs.qmd%22){target="blank"}) this
isn't an issue.

##### Random effects

Our final option takes a new approach. It considers the units we
measured as simply a sample from a larger population. Using that
background, we use the information from the units to consider the
distribution of sample effects we might see. The impact of unit is then
considered a random-effect. For this to work, you probably want 5+
levels of the unit variable. This is because we are using the means to
estimate variance (confusing?). For factors with \<5 levels, random
effects likely offer no benefit [@gomes2022]

## Next steps

These methods can be extended to other models that are used when linear
model assumptions are not met, which is the focus of the next chapter.
