---
title: The one where we introduce hypothesis testing via binomial tests
subtitle: Where we meet the p-value!
bibliography: references.bib
---

In this chapter will build on our previous exploration of estimation by
considering the world of hypothesis testing. These are different but
related ideas, and we'll end the section showing why. Along the way we
will introduce the p-value. We will do all this while considering
binomial tests, which are some of the simplest data we will see.

## Example: Does age of birds impact their likelihood of colliding with glass?

Let's start with an example. Klem [@danielklem1989] wanted to know if
various factors (e.g., age, sex) of birds impacted the probability they
would collide with glass windows. He collected data from several areas.
In one of his samples, he found 18 purple finches collided with glass
windows. 9 of these were in their hatching year (we'll call them
younger), and 9 were older. Is there any evidence that age impacts the
probability of a purple finch colliding with the glass?

[![Cephas, CC BY-SA 3.0
\<https://creativecommons.org/licenses/by-sa/3.0\>, via Wikimedia
Commons](/images/Carpodacus_purpureus_CT3.jpg){fig-alt="Purple Finch (Carpodacus purpureus) male, Cap Tourmente National Wildlife Area, Quebec, Canada."}](https://commons.wikimedia.org/wiki/File:Carpodacus_purpureus_CT3.jpg)

What have we done with data like this so far? You should know to
calculate the proportion of each category impacting the probability of
either category being represented in the sample. For example, since
there were 9 older birds and 18 total, the proportion of older birds in
the sample was:

```{r}
9/18
```

We could also graph this, but it wouldn't be very interesting:

```{r}
library(ggplot2)
finch_data <- data.frame(age = c("younger", "older"), collisions = c(9,9))
ggplot(finch_data, aes (x=age, y = collisions))+
  geom_col()+
  labs(x="Age", y= "Collisions", title = "No apparent difference in sample based on age")
```

And although we haven't discussed it, you should understand we could
develop a confidence interval fr this type of data (we'll do so below).
That would tell us the range of proportions we might typically expect.
But does that really answer the question of whether age impacts
likelihood of colliding with glass?

## Welcome to hypothesis testing

To answer that question, we have to move to hypothesis testing. This
approach focuses on if a given value we found in the data (we'll call it
a signal) is really different (or *significantly* different) from what
we would expect to see for a a given set of circumstances given the
sampling error we now know to expect when we sample.

The *given set of circumstances* are described by a *null* hypothesis
(this is why this approach is sometimes called NHST, or null hypothesis
significance testing). We often abbreviate this as H~o~. Let's start by
comparing this to estimation. Given our data, we could develop a 95%
confidence interval (theoretically) that you should now understand will
capture the true signal of the data about 95% of the time (here we are
using proportion as opposed to mean, but it works). That's a slightly
different approach than asking if the true mean is equal to a given
number, which is what hypothesis testing asks. Both deal with sampling
error and explain why we can't simply say an estimate being different
than a given value proves there is a difference (make sure you
understand why!).

For hypothesis testing in general, we again generate a known population
that we draw from multiple times (should sound familiar), but this time
the population parameters are set by a null hypothesis. Then we compare
the spread of signals from those multiple draws (which exist due to
sampling error!) to what we actually observed to determine how likely
our draw was given the null hypothesis was true. If it's unlikley to
have occured by chance under the null hypothesis, we consider that
evidence the null hypothesis is not correct and (eventually) reject it.

You can typically think of a null hypothesis as a hypothesis of no
difference, affect, or relationship. Let's walk through this with our
bird example, where our null hypothesis would state age (measured as a
category here!) has no impact on collisions. Given that, what would we
expect to see in our sample?

This is a tricky question (that I chose on purpose!). Many approaches to
this question start with a 50/50 expectation (like flipping a coin), but
I've found that confuses students into thinking that is *always* the
answer. Instead, think about what we would expect to see if age had no
impact on collisions. We would not necessarily expect a 50/50 split in
older and younger birds *because that may not be what the population
looks like*. In fact, previous research has suggested the population is
split closer to 3:1, with 3 older birds for every younger bird. This
means if age has no impact on collisions, we should see about (due to
sampling error!) 3 older birds for every younger bird in our samples of
birds that hit glass.

What did we actually see? We saw 5 old birds and 5 young birds. That is
not a 3:1 split, but its also a small sample size. If we had a
population with a 3:1 split and randomly selected 10 birds from it, how
rare would it be to get 5 younger and 5 older birds? That' (close) to
what we are asking.

In this case, our null hypothesis is comparing our signal to a set
value. This is common when we measure a single group and want to compare
it to something. So our null hypothesis could be written as conceptually
as age does not impact the probabilty a bird collides with glass.
However, its often better (in order to connect it to tests!) to write it
using numbers. In this case, we could write

$$
H_O: p=.75
$$

where *p* is the probability of a bird in our sample being old. Note we
could instead focus on young birds and get:\
$$
H_O: p=.25
$$

We also have alternative hypotheses (abbreviated H~A~)to accompany each
of these. Our alternative is just the opposite of the null. Together,
they encompass all the probability space. **It is usually just as simple
as switching signs**. For example, if we focus on older birds, we get

$$
H_O: p=.75 \\
H_A: p \neq .75
$$

The above ideas stay the same for all NHST approaches! We always use the
null hypothesis to generate a "known" population (sometimes called the
*null population*, draw samples from it, and then compare it to what we
actually observed. What changes based on data type is how we generate
the sample and multiple draws.

## Binomial data

This example focuses on data that only has 2 outcomes (young and old in
our example). That is known as *binomial* data. **For any type of data,
we can simulate a distribution under the null hypothesis**. For this
example, we could put 4 pieces of paper in a hat, 3 labelled *older* and
1 labelled *younger*. We can then draw a sample of 18 (the number we
actually observed) by drawing a piece of paper, writing down what it
says, returning it to the hat, and repeating the process 9 more times to
get single sample. For each sample, you could then calculate the
observed proportion of older birds. You could visualize the spread of
those results using a histogram. It's important to realize this is
*doable* without a computer (think it through), but it would take a lot
of time because you need a lot of samples (**we'll come back to this**).

For now, let's do it with the computer. Let's also take a shortcut:
Instead of younger and older, let's label the pieces of paper 0 and 1.
Then we can sum the draws and divide by 10 to get the proportion (make
sure you understand why!). For now, let's do a 1000 random draws of 18.

```{r}
choices <- c(rep(0,1),rep(1,3))
number_of_draws <- 18
number_of_simulations <- 1000

sampling_experiment<- data.frame("observed_proportion" = rep(NA, number_of_simulations))
for(i in 1:number_of_simulations){
sampling_experiment$observed_proportion[i] = sum(sample(choices,number_of_draws, replace=T))/number_of_draws
}
```

Let' s take a look at the first few draw

```{r}
head(sampling_experiment$observed_proportion)
```

Note we see some variation. Also note it is *impossible* to get a
proportion of .75. Why? We only sampled 18 individuals, so we can't get
any outcomes that aren't some form of a whole number less than 18
divided by 18. This seems simple, but it's a reminder that your signal
being different than your hypothesized value is not sufficient to reject
the null hypothesis!

Now let's plot the observed proportions:

```{r}
ggplot(sampling_experiment,
              aes(x=observed_proportion)) +
  geom_histogram( fill="blue", color="black") +
  labs(title="Observed proportions from 1000 random draws",
       x= "Proportion",
       y= "Frequency")

```

Just looking at this, it seems getting a proportion of .5 is unlikely.
It only occurred
`r length(sampling_experiment[sampling_experiment$observed_proportion == .5,])`
times. However, we also need to note how often *more* extreme outcomes
occurred. Why?

More extreme values (the same or further distance away from the
hypothesized value as our observed signal were) are also useful in
considering if the null hypothesis is valid. When we move to continuous
distributions, it's also impossible to get a certain value (as mentioned
in the probability section).

In this example, our observed proportion was .5. That's .25 away from
the value under the null hypothesis (.75), so we should all simulations
that were . 5 or less or 1 or more. That only happened
`r length(sampling_experiment[sampling_experiment$observed_proportion <= .5|sampling_experiment$observed_proportion >= 1,])`
times. So, in taking 1000 random draws from our null population, we only
saw what we actually observed (or something more extreme)
`r length(sampling_experiment[sampling_experiment$observed_proportion <= .5|sampling_experiment$observed_proportion >= 1,])*.001`%
of the time.

*This is a p-value*. Don't get confused! We will get p-values from
multiple tests, but the binomial distribution also has a *p* parameter
(the proportion). They are not the same.

Explaining p-values is hard!

```         
{{< video https://www.youtube.com/watch?v=bz9nzc-jVEE&t=4s >}}
```

A smaller p-value therefore means it is less likely to obtain your
observed signal, or something more extreme, by chance when the null
hypothesis is true. undefined
