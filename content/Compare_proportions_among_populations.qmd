---
title: Comparing proportions among groups
subtitle: Moving to multiple populations!
bibliography: references.bib
---

Now that we've covered hypothesis testing for both discrete and
continous data, we'll extend these ideas to compare differences among
groups. In addition considering these differences, the same test we'll
let us consider if proportions follow a given ratio.

## Example: Back to the birds

Let's return to our bird example @klem1989. We previously found that
purple finches did not strike windows at proportions that might be
predicted by population demographics using a binomial test. However,
what if instead we wanted to compare the collision rate of old vs young
birds among several species?

[![Cephas, CC BY-SA 3.0
\<https://creativecommons.org/licenses/by-sa/3.0\>, via Wikimedia
Commons](/images/Carpodacus_purpureus_CT3.jpg){fig-alt="Purple Finch (Carpodacus purpureus) male, Cap Tourmente National Wildlife Area, Quebec, Canada."}](https://commons.wikimedia.org/wiki/File:Carpodacus_purpureus_CT3.jpg)

Let's start simple and just compare purple finches and dark-eyed juncos.
Klem's sample of finches totalled 18, with 9 being older (after hatching
year). For juncos, 4 of 11 sampled birds were older. We could put this
data in a table.

+----------+----------+----------+----------+
| **Ob     | *        | *        | Totals   |
| served** | *Finch** | *Junco** |          |
+----------+----------+----------+----------+
| **Old**  | 9        | 4        | 13       |
+----------+----------+----------+----------+
| *        | 9        | 7        | 16       |
| *Young** |          |          |          |
+----------+----------+----------+----------+
| Totals   | 18       | 11       |          |
+----------+----------+----------+----------+

First, we could plot our data

```{r}
birds_original <- data.frame(Age = c("Old","Young","Old", "Young"),
                      Species = c("Junco", "Junco", "Finch", "Finch"),
                      Number = c(4, 7, 9, 9))
library(ggplot2)
ggplot(birds_original, aes(x= Species, y = Number)) +
  geom_col(aes(fill = Age)) + 
  xlab("Species") +
  ylab("Frequency") +
  ggtitle("Age at collision for juncos and finches")
```

Given the different sampling sizes, a mosaic plot might help in visually
comparing ratios.

```{r}
ggplot(birds_original, aes(x= Species, y = Number)) +
  geom_col(aes(fill = Age), position = "fill")

```

Before we test this, we need to decide on an hypothesis. Although both
species were predicted to occur at 3:1 ratios in the wild, that's not
what we are considering here. Instead, we want to know if the likelihood
of old vs young birds being in our samples differed for the species. If
so, it could be because the age structure of the focal populations are
different or because the birds differ in their relationship to glass at
different ages. However, we are still testing a distribution-based
parameter.

$$
\begin{split}
H_O: p_{finches} = p_{juncos} \\  
H_A: p_{finches} \neq p_{juncos} \\
where \ p \ is \ likelihood \ of \ sampled \ bird \ being \ older \\ 
\end{split}
$$

Put another way,we want to test if *p* is independent of species.

$$
\begin{split}
H_O: Probability \ of \ older \ bird \ hitting \ window \ is \ independent \ of \ species \\  
H_A: Probability \ of \ older \ bird \ hitting \ window \ is \ dependent \ of \ species \\  
\end{split}
$$

This formulation is important, because it helps form our predictions
under the null hypothesis. What would we expect if *p* did not differ
among species? If age and species were independent, we could expect

$$
Pr[ Old \ AND \ Given \ species\ use] = Pr[given \ species] * Pr[Old]
$$

Since we have 13/29 birds are old, we should expect

+-------+-------+-------+-------+
| **    | **Fi  | **Ju  | T     |
| Expec | nch** | nco** | otals |
| ted** |       |       |       |
+-------+-------+-------+-------+
| -     | 18\   | 11\   | 13    |
|  \*Ol | \*    | \*    |       |
| d\*\* | 13/29 | 13/29 |       |
+-------+-------+-------+-------+
| **Y   | 18\   | 11\   | 16    |
| o     | \*    | \*    |       |
| ung** | 16/29 | 16/29 |       |
+-------+-------+-------+-------+
| T     | 18    | 11    | 29    |
| otals |       |       |       |
+-------+-------+-------+-------+

In order to carry out a sampling experiment to consider noise from this
expected outcome, we have to determine the p parameter to use for our
population. This is because under the null hypothesis, there is only one
population- any observed difference is just due to chance!

However, we have an issue - we don't know p. In our binomial experiment
it was set by our null hypothesis. Now we are comparing *p* among
species, but that doesn't set a population distribution.

To fix this, we go back to our normal approximations. We have shown for
large sample sizes the binomial distribution follows the central limit
theorem.

```{r}
sample_size=c("1","5","10", "20", "40", "80")
number_of_simulations <- 1000
sampling_experiment <- setNames(data.frame(matrix(ncol = length(sample_size), nrow = number_of_simulations)), sample_size)

for(k in 1:length(sample_size)){
for(i in 1:number_of_simulations){
sampling_experiment[i,k] = rbinom(n=1, size=as.numeric(sample_size[k]), prob=.7)/as.numeric(sample_size[k])
}
}
library(reshape2)
sampling_experiment_long <- melt(sampling_experiment, variable.name = "Sample_size", value.name = "mean")
sampling_experiment_long$Sample_size <- factor(sampling_experiment_long$Sample_size, levels =c("1","5","10", "20", "40", "80"))
levels(sampling_experiment_long$Sample_size) <- paste("Sample size of ", levels(sampling_experiment_long$Sample_size))

ggplot(sampling_experiment_long,aes(x=mean)) +
  geom_histogram(color="black") +
  labs(title=paste("Observed means from ", number_of_simulations, " random draws"),
       subtitle = "Binomial distribution, p=.7",  
       x= "Mean",
       y= "Frequency")+
    facet_wrap(~Sample_size, nrow = 2)
```

So, we can replace the binomial distribution in our sampling experiment
with a normal population. To use this approach, we estimate a value for
*p*, $\hat{p}$, from the data, and let

$$
\begin{split}
\mu = Np \\
\sigma_\mu =\sqrt{Np(1-p)}
\end{split}
$$

We then draw only 1 draws this distribution? Why only 1? Because we need
to keep the sample sizes the same, so the row and column totals are set!
Remember this for a moment. After we draw 1 number, we fill in the rest.

```{r, eval=F}
num_of_simulations <- 10000
num_of_buckets <- 4
chi_sq_stats <- data.frame(chisq = rep(NA, num_of_simulations))
for(i in 1:num_of_simulations){
simulation <- table(sample(LETTERS[1:num_of_buckets], n, replace=T))
chi_sq_stats$chisq[i] <- chisq.test(simulation)$statistic
}
library(ggplot2)
ggplot(chi_sq_stats, aes(x=chisq)) +
  geom_histogram(aes(y=..count../sum(..count..)), fill = "orange")+
    stat_function(fun = dchisq, args = list(df = 2),color = "green")
```

Once we carry out the sampling experiment, we can Z-transform our cell
data(because they are normal now!). The results for a single cell would
follow a N(0,1) distribution (the Z). If we wanted, we could square
these outcomes (which would then follow a $\chi^2$ distribution, by
definition), and, since we have 4 cells, add them. The resulting variate
would follow a $\chi^2$ distribution with 1 degree of freedom (since we
drew 1 numbers for the free "cell" in our table). Finally, because of
all the p's above, we could actually rewrite all of this as

$$
V=\sum{\frac{{Observed-Expected}^2}{Expected}}
$$

## The $\chi^2$ test

The resulting test is called a $\chi^2$ test. Note this takes
count-based data and uses a continuous distribution to describe it, so
it's an approximate test.The degrees of freedom associated with this
test are based on the number of free cells (or, alternatively, the
number of cells minus the parameters you had to fill in!). This
typically can be calculated as (# of columns -1)\*(# of rows -1). We can
carry out this test in R using the *chisq.test* function.

This function requires a matrix of aggregated values (counts for each
cell). Currently we have a data frame (birds). To make this work, we
have a few options.

We can input the data directly as a matrix, specifying the string, the
number of rows and columns, and how we entered the data in regards to
rows (remember ?chisq.test)

```{r}
chisq.test(matrix(c(9,4,9,7), 2, 2, byrow=T))
```

If the data is in *wide* data frame (meaning more than one measured
outcome per row, so measuring the young and old as columns in the data
frame) or we make it look like that, we can use the data directly from
the data frame. Consider the difference in format. This is long data
(one measure per row):

```{r}
birds_original
```

and this wide

```{r}
library(reshape2)
birds_wide <- dcast(birds_original, Age~Species)
birds_wide
```

We can make the matrix with wide data

```{r}
chisq.test(matrix(c(birds_wide$Junco, birds_wide$Finch), nrow = 2))
```

For our 2x2 table, notice the order of input does not matter (because it
wouldn't impact the expected values. Consider

```{r}
chisq.test(matrix(c(birds_wide$Finch, birds_wide$Junco), nrow = 2))
```

Using *cbind* is also an option.

```{r}
chisq.test(cbind(birds_wide$Finch, birds_wide$Junco))
```

As long as we specify the table, we are ok. Note each of these tests
notes 1 df and a p-value greater than .05. This would suggest we should
fail to reject the null hypothesis.

Each result also tells you this is an approximation (as we already
noted!) but also indicates it may be incorrect? Why?

In order for our normal approximation to work, we need large samples and
a $\hat{p}$ that is not near 0 or 1. Together, these mean our expected
values for most cells (actually, \>80%) can not be less than 5., and no
cell can have an expected value of less than 1. If these assumption are
not met(or are close), R will warn us. We can check expected values
using

```{r}
chisq.test(matrix(c(9,4,9,7), 2, 2, byrow=T))$expected
```

In this case, 25% of the cells (1/4) has an expected value of less than
5.

## Other options

### Fisher's test

If this is the case for a 2x2 table, we can use a Fisher's test instead

```{r}
fisher.test(matrix(c(9,4,9,7), 2, 2, byrow=T))
```

As opposed to resampling from a null distribution, Fisher's test
considers all (or lots) of ways the data could be re-arranged (or
*permuted*) and then computes a p-value using that approach. This means
Fisher's test works for any sample size. It is an exact test *if* all
possible combinations are considered (but they rarely are).

Notice the output reports the odds ratio. This ratio is found by
dividing odds in one group by odds in another (thus a ratio). Odds are
the probability of one outcome over another. For our data, this could be
considered (old/young(finches)) divided by (old/young(juncos)),
(9/9)/(4/7)=63/36. This is close to what we saw in the output; slight
differences occur since the *fisher.test* function returns a conditional
estimate of the odds ratio.

Note odds differ from relative risks, which compare the probability of
an event occurring (or not) among 2 groups. The results are similar for
rare events (think about why!) but not for common events. For more, see
@altman1998 @davies1998

### G test

Another option, the G test, uses a slightly different approach as well.
Instead of resampling, the test uses likelihood to compare outcomes.
Likelihood asks how likely we were to observed a given set of data given
parameter values. If you calculate the likelihood under multiple (or
all) parameter values, you can determine which value is most likely and
get a p-value since ratio of likelihood values follow a $\chi^2$
distribution. We will return to likelihood-based approaches later, as
they can be used for any dataset we can generate a model for.

```{r}
library(DescTools)
GTest(x = matrix(c(9,7,9,4), 2, 2, byrow = T))

```

## What about more than 2 groups?

These ideas can be extended to compare more than 2 groups with a few
important caveats.

-   The Fisher test is even less exact since all permutations of the
    data can seldom be explored

-   **More importantly**, if we reject the null hypothesis we need to do
    follow-up tests.

Let's explore this idea with the Klem data. In addition to considering
impacts of age, Klem also recorded the sex (coded as male/female) for
each bird. He had data on 4 species.

+-----------------+-----------------+-----------------+--------+--------+
| **Observed**    | **Finch**       | **Junco**       | **Card | **R    |
|                 |                 |                 | inal** | obin** |
+-----------------+-----------------+-----------------+--------+--------+
| **Male**        | 6               | 5               | 7      | 7      |
+-----------------+-----------------+-----------------+--------+--------+
| **Female**      | 12              | 7               | 11     | 3      |
+-----------------+-----------------+-----------------+--------+--------+

We can use the same approaches to test this.

```{r}
chisq.test(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))
```

Note our sample size is still a potential issue, but only 1/8 cells has
a predicted value less than 1.

```{r}
chisq.test(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))$expected
```

which means the approximation is fine. We could use the other tests if
we preferred.

```{r}
fisher.test(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))
GTest(matrix(c(6,5,7,7,12,7,11,3), nrow=2, byrow=T))
```

Regardless, we see all p\>.05. What does this mean?

### Post-hoc comparisons: Controlling for the FWER

When we compared one group to a set value or two groups to each other,
this was easy: it meant our focal parameter was the same between the
groups or between expected and observed values. For more than 2 groups,
it means the parameters is also means the parameter of interest does not
differ among the groups. In other words, our null hypothesis is

$$
H_O: p_{finch} = p_{junco} = p_{cardinal} = p_{robin}, where p is proportion of population that is male
$$

Here, this means all species have similar male/female ratios (at least
given our samples). This is an example of a very useful *insignificant*
result. This study would have been interesting regardless of outcome.

However, let's imagine we had data on another species (catbirds in the
table below).

+----------+----------+----------+-----------+-----------+-----------+
| **Ob     | *        | *        | **C       | **Robin** | **C       |
| served** | *Finch** | *Junco** | ardinal** |           | atbirds** |
+----------+----------+----------+-----------+-----------+-----------+
| **Male** | 6        | 5        | 7         | 7         | 30        |
+----------+----------+----------+-----------+-----------+-----------+
| **       | 12       | 7        | 11        | 3         | 7         |
| Female** |          |          |           |           |           |
+----------+----------+----------+-----------+-----------+-----------+

When we run the test (notice I immediately checked expected values, or
assumptions, which is a good habit to get into)

```{r}
chisq.test(matrix(c(6,5,7,7,30,12,7,11,3,7), nrow=2, byrow=T))
chisq.test(matrix(c(6,5,7,7,30,12,7,11,3,7), nrow=2, byrow=T))$expected
```

We now have a significant p-value (.002). What does this mean now?

A significant p-value from a multi-population test means the parameter
is not the same for **all** groups. However, it does not necessarily
mean the parameter is different for every group. Remember, our null
hypothesis is

$$
H_O: p_{finch} = p_{junco} = p_{cardinal} = p_{robin}, where p is proportion of population that is male
$$

We would reject this if we have evidence **any** of these qualities are
not true.

In the above example, we know there was no difference before we added
catbirds. So, we might assume the only "different" group in relation to
the male/female ratio is catbirds. But we need to test that. In other
words, after we show using an overall, or *omnibus*, test that there is
a different among populations, we need to determine which ones actually
defer from the others. We do this using post-hoc specific comparisons.
We cantechnically choose which comparisons to focus on. For example, you
can do compare all possible pairs or just certain combinations. Why
would this matter?

The answer has to do with family-wise error rate (FWER). Remember, for
every test we run we have an $\alpha$% chance of a type 1 error. If we
run many tests, the likelihood of making a type 1 error increases (the
rate of increase depends on how independent the tests are, but we need
to control for it.

[![XKCD 882:
Significant](https://imgs.xkcd.com/comics/significant.png)](https://xkcd.com/882)

For this reason, we modify our "used" $\alpha$ for our post-hoc tests.
There are many approaches to doing this, but they all depend on how many
tests we run - so the more post-hoc comparisons we include, the harder
it may be to find a significant difference among focal pairs.

To illustrate this, we will first use a very simple method that is no
longer recommended but is useful as a starting point. One options is to
control the FWER by dividing $\alpha$ by the number of post-hoc tests we
intend to run. For the above example, if we do all pairs comparisons we
would be running 10 comparison (4+3+2+1...). So instead of using .05 as
a cutoff, we would use .005.

First, it helps to make a table with row and column names (which are
slightly different than headers and very different than a column of
names in R).

```{r}
bird_ratio_table <- matrix(c(6,5,7,7,30,12,7,11,3,7), nrow=2, byrow=T)
colnames(bird_ratio_table) <- c("Finch", "Junco", "Cardinal", "Robin", "Catbird")
rownames(bird_ratio_table) <- c("Male", "Female")

```

Then we can run the test with the *pairwiseNominalIndependence* function
from the **rcompanion** package. Note the function needs a table or
matrix and to know which method to use to compare rows or columns.
Looking at the table

```{r}
bird_ratio_table
```

Let' s us see we want to compare columns.

```{r}

library(rcompanion)
bonf_correct <- pairwiseNominalIndependence(bird_ratio_table, compare="col", method = "bonf")
bonf_correct

```

The test then shows all-pair comparisons with regular (what we should
compare to .005 now, but we don't usually know that!) and adjusted
p-values (which have compensated for multiple tests so we can use our
normal .05 cutoff- use this one!) for each test we have covered (you
should use a post-hoc that matches what you did for the overall, or
*omnibus*, comparison).

You can order and display these differently if it helps. For example, if
we used the $\chi^2$ test.

```{r}
bonf_correct[order(bonf_correct$p.adj.Chisq), c("Comparison", "p.adj.Chisq")]
```

We see that catbirds different from finches and cardinals in the
proportion of males and females, while all other species do not differ.
Note the un-adjusted p-value for this comparison pair is the same we
would have found from just comparing the two groups

```{r}
chisq.test(matrix(c(6,30,12,7), nrow=2, byrow=T))
bonf_correct[bonf_correct$Comparison=="Finch : Catbird", "p.Chisq"]

```

However, this comparison is very conservative. Many other options exist,
and we will explore two here.

The sequential Bonferroni, or Holm's, method, allocates your $\alpha$ to
accept as many tests as signficant as possible while still controlling
for the FWER. To do this, it orders the post-hoc tests by p-value,
smallest to largest. It then rejects the null hypothesis attached to the
smallest p-value and subtracts that p-value from $\alpha$. It continues
to do this until $\alpha$ is too small to reject the next smallest
p-value. I think of it as buying p-values with $\alpha$.

We can use this approach by simply changing the method. Let's also order
our results again for viewing.

```{r}
bonf_correct <- pairwiseNominalIndependence(bird_ratio_table, compare="col", method = "holm")
bonf_correct[order(bonf_correct$p.adj.Chisq), c("Comparison", "p.adj.Chisq")]
```

```{bonf_correct <- pairwiseNominalIndependence(bird_ratio,}

```

```{r}

holm_correct <- pairwiseNominalIndependence(travel_table, method = "holm")
holm_correct[order(holm_correct$p.adj.Fisher),]
fdr_correct <- pairwiseNominalIndependence(travel_table, method = "fdr")
fdr_correct[order(fdr_correct$p.adj.Fisher),]
```

```{r}
knitr::knit_exit()
```

```{r}
num_of_simulations <- 10000
num_of_buckets <- 4
prop_est <- 13/29
n <- 29
mean_est <- n * prop_est
sigma_est <- sqrt(n*prop_est*(1-prop_est))

chi_sq_stats <- data.frame(chisq = rep(NA, num_of_simulations))
for(i in 1:num_of_simulations){
simulation <- table(sample(LETTERS[1:num_of_buckets], n, replace=T))
chi_sq_stats$chisq[i] <- chisq.test(simulation)$statistic
}
library(ggplot2)
ggplot(chi_sq_stats, aes(x=chisq)) +
  geom_histogram(aes(y=..count../sum(..count..)), fill = "orange")+
    stat_function(fun = dchisq, args = list(df = 2),color = "green")



```

```{r}
n <- 29
num_of_simulations <- 10000
num_of_buckets <- 4
chi_sq_stats <- data.frame(chisq = rep(NA, num_of_simulations))
for(i in 1:num_of_simulations){
simulation <- table(sample(LETTERS[1:num_of_buckets], n, replace=T))
chi_sq_stats$chisq[i] <- chisq.test(simulation)$statistic
}
library(ggplot2)
ggplot(chi_sq_stats, aes(x=chisq)) +
  geom_histogram(aes(y=..count../sum(..count..)), fill = "orange")+
    stat_function(fun = dchisq, args = list(df = 2),color = "green")
```

and, if we have large enough sample sizes, carry out a sampling
experiment

What have we done with data like this so far? You should know to
calculate the proportion of each category impacting the probability of
either category being represented in the sample. For example, since
there were 9 older birds and 18 total, the proportion of older biards in
the sample was:

```{r}
9/18
```

We could also graph this, but it wouldn't be very interesting:

```{r}
library(ggplot2)
finch_data <- data.frame(age = c("younger", "older"), collisions = c(9,9))
ggplot(finch_data, aes (x=age, y = collisions))+
  geom_col()+
  labs(x="Age", y= "Collisions", title = "No apparent difference in sample based on age")
```

And although we haven't discussed it, you should understand we could
develop a confidence interval fr this type of data (we'll do so below).
That would tell us the range of proportions we might typically expect.
But does that really answer the question of whether age impacts
likelihood of colliding with glass?

## Welcome to hypothesis testing

To answer that question, we have to move to hypothesis testing. This
approach focuses on if a given value we found in the data (we'll call it
a signal) is really different (or *significantly* different) from what
we would expect to see for a a given set of circumstances given the
sampling error we now know to expect when we sample.

The *given set of circumstances* are described by a *null* hypothesis
(this is why this approach is sometimes called NHST, or null hypothesis
significance testing). We often abbreviate this as H~o~. Let's start by
comparing this to estimation. Given our data, we could develop a 95%
confidence interval (theoretically) that you should now understand will
capture the true signal of the data about 95% of the time (here we are
using proportion as opposed to mean, but it works). That's a slightly
different approach than asking if the true mean is equal to a given
number, which is what hypothesis testing asks. Both deal with sampling
error and explain why we can't simply say an estimate being different
than a given value proves there is a difference (make sure you
understand why!).

For hypothesis testing in general, we again generate a known population
that we draw from multiple times (should sound familiar), but this time
the population parameters are set by a null hypothesis. Then we compare
the spread of signals from those multiple draws (which exist due to
sampling error!) to what we actually observed to determine how likely
our draw was given the null hypothesis was true. If it's unlikley to
have occured by chance under the null hypothesis, we consider that
evidence the null hypothesis is not correct and (eventually) reject it.

You can typically think of a null hypothesis as a hypothesis of no
difference, affect, or relationship. Let's walk through this with our
bird example, where our null hypothesis would state age (measured as a
category here!) has no impact on collisions. Given that, what would we
expect to see in our sample?

This is a tricky question (that I chose on purpose!). Many approaches to
this question start with a 50/50 expectation (like flipping a coin), but
I've found that confuses students into thinking that is *always* the
answer. Instead, think about what we would expect to see if age had no
impact on collisions. We would not necessarily expect a 50/50 split in
older and younger birds *because that may not be what the population
looks like*. In fact, previous research has suggested the population is
split closer to 3:1, with 3 older birds for every younger bird. This
means if age has no impact on collisions, we should see about (due to
sampling error!) 3 older birds for every younger bird in our samples of
birds that hit glass.

What did we actually see? We saw 5 old birds and 5 young birds. That is
not a 3:1 split, but its also a small sample size. If we had a
population with a 3:1 split and randomly selected 10 birds from it, how
rare would it be to get 5 younger and 5 older birds? That' (close) to
what we are asking.

In this case, our null hypothesis is comparing our signal to a set
value. This is common when we measure a single group and want to compare
it to something. So our null hypothesis could be written as conceptually
as age does not impact the probabilty a bird collides with glass.
However, its often better (in order to connect it to tests!) to write it
using numbers. In this case, we could write

$$
H_O: p=.75
$$

where *p* is the probability of a bird in our sample being old (for a
single draw), or what we expect on average over larger samples (a
proportion!). Remember, to find a proportion, you count the number of
samples that fall in a given group and divided that by the total number
sampled. Alternatively, you can assign a score of *0* for values that
are not in the focal group and a score of *1* to samples that are - the
average of these scores will give you the proportion.

Note we could instead focus on young birds and get:\
$$
H_O: p=.25
$$

We also have alternative hypotheses (abbreviated H~A~)to accompany each
of these. Our alternative is just the opposite of the null. Together,
they encompass all the probability space. **It is usually just as simple
as switching signs**. For example, if we focus on older birds, we get

$$
\begin{split}
H_O: p=.75 \\  
H_A: p \neq .75
\end{split}
$$

The above ideas stay the same for all NHST approaches! We always use the
null hypothesis to generate a "known" population (sometimes called the
*null population*, draw samples from it, and then compare it to what we
actually observed. What changes based on data type is how we generate
the sample and multiple draws.

## Binomial data

This example focuses on independent data points (one does not impact any
others) that can be divided into 2 outcomes (young and old in our
example). That is known as *binomial* data (so if data can be divided
into two categories, we call it *binomial data*). A special case of
binomial data exists when we only get one organism in our sample (e.g.,
one bird, one coin flip). We call this *Bernoulli* data.

**For any type of data, we can simulate a distribution under the null
hypothesis**. For this example, we could put 4 pieces of paper in a hat,
3 labelled *older* and 1 labelled *younger*. We can then draw a sample
of 18 (the number we actually observed) by drawing a piece of paper,
writing down what it says, returning it to the hat, and repeating the
process 9 more times to get single sample. For each sample, you could
then calculate the observed proportion of older birds. You could
visualize the spread of those results using a histogram. It's important
to realize this is *doable* without a computer (think it through), but
it would take a lot of time because you need a lot of samples (**we'll
come back to this**).

For now, let's do it with the computer. Let's also take a shortcut:
Instead of younger and older, let's label the pieces of paper 0 and 1.
We will also call the 0's failures and the 1's successes. Then we can
sum the draws and divide by 10 to get the proportion of successes (make
sure you understand why!). For now, let's do a 1000 random draws of 18.

```{r}
set.seed(42)
choices <- c(rep(0,1),rep(1,3))
number_of_draws <- 18
number_of_simulations <- 1000

sampling_experiment<- data.frame("observed_proportion" = rep(NA, number_of_simulations))
for(i in 1:number_of_simulations){
sampling_experiment$observed_proportion[i] = sum(sample(choices,number_of_draws, replace=T))/number_of_draws
}
```

Let' s take a look at the first few draw

```{r}
head(sampling_experiment$observed_proportion)
```

Note we see some variation. Also note it is *impossible* to get a
proportion of .75. Why? We only sampled 18 individuals, so we can't get
any outcomes that aren't some form of a whole number less than 18
divided by 18. This seems simple, but it's a reminder that your signal
being different than your hypothesized value is not sufficient to reject
the null hypothesis!

Now let's plot the proportions from our sampling experiment:

```{r}
ggplot(sampling_experiment,
              aes(x=observed_proportion)) +
  geom_histogram(color="black") +
  labs(title="Observed proportions from 1000 random draws",
       x= "Proportion",
       y= "Frequency")

```

Just looking at this, it seems getting a proportion of .5 is unlikely.
It only occurred
`r length(sampling_experiment[sampling_experiment$observed_proportion == .5,])`
times. However, we also need to note how often *more* extreme outcomes
occurred. Why?

More extreme values (the same or further distance away from the
hypothesized value as our observed signal were) are also useful in
considering if the null hypothesis is valid. When we move to continuous
distributions, it's also impossible to get a certain value (as mentioned
in the probability section).

In this example, our observed proportion was .5. That's .25 away from
the value under the null hypothesis (.75), so we should all simulations
that were . 5 or less or 1 or more. That only happened
`r length(sampling_experiment[sampling_experiment$observed_proportion <= .5|sampling_experiment$observed_proportion >= 1,])`
times. So, in taking 1000 random draws from our null population, we only
saw what we actually observed (or something more extreme)
`r length(sampling_experiment[sampling_experiment$observed_proportion <= .5|sampling_experiment$observed_proportion >= 1,])*.001`%
of the time.

```{r}

sampling_experiment$Proportion = ifelse(sampling_experiment$observed_proportion <= .5, 
                                  '<= .5', ifelse(sampling_experiment$observed_proportion >= 1, '>.5 & < 1', '>= 1'))




ggplot(sampling_experiment,
              aes(x=observed_proportion, fill=Proportion)) +
  geom_histogram(color="black") +
  labs(title="Observed proportions from 1000 random draws",
       x= "Proportion",
       y= "Frequency")

```

Or to think about as or more extreme..

```{r}
sampling_experiment$Proportion2 = ifelse(abs(sampling_experiment$observed_proportion-.75) >= abs(9/18-.75), 'as or more extreme', 'not as or more extreme')

ggplot(sampling_experiment,
              aes(x=observed_proportion, fill=Proportion2)) +
  geom_histogram(color="black") +
  labs(title="Observed proportions from 1000 random draws",
       x= "Proportion",
       y= "Frequency", 
       fill = "Proportion")
```

## Welcome to the p-value

*This is a p-value*. Don't get confused! We will get p-values from
multiple tests, but the binomial distribution also has a *p* parameter
(the proportion). They are not the same.

Explaining p-values is hard! You can see some statisticians try to
explain the concept
[here](https://www.youtube.com/watch?v=bz9nzc-jVEE&t=4s){target="_blank"}.

A smaller p-value therefore means it is less likely to obtain your
observed signal, or something more extreme, by chance when the null
hypothesis is true. Traditionally, a p-value of less than .05 is thought
to be sufficient evidence to reject the null hypothesis. This comparison
value is sometimes called the $\alpha$ (alpha), or significance, level.
So if our p-value is \< .05, we often say we have significant evidence
against H~O~. While we now often get specific p-values from software,
historically people used tables to find ranges (less than .05, for
example).

## Understand what this implies!

Note our p-value is the probability we would get a signal like we
observed by chance *if* the null hypothesis was true. This means for an
$\alpha$ of .05,we would expect to see something this extreme by chance
1 time out of 20! In other words, we can have errors. Think about it
this way:

+----------+----------+----------+
|          | -   \* B |          |
|          |     i    |          |
|          | ological |          |
|          |     r e  |          |
|          |     a    |          |
|          | lity\*\* |          |
+----------+----------+----------+
| \        | **H~O~   | **H~O~   |
|          | True**   | False**  |
| -   \*   |          |          |
| Decision |          |          |
|          |          |          |
|   (based |          |          |
|     on   |          |          |
|          |          |          |
| analysis |          |          |
|     of   |          |          |
|          |          |          |
|   sample |          |          |
|     d    |          |          |
| ata)\*\* |          |          |
+----------+----------+----------+
| **Reject | Type I   | Power    |
| H~O~**   | error    | (1-      |
|          | (P\[ɑ\]) | $\beta$  |
|          |          | )        |
+----------+----------+----------+
| **Do not | Correct  | Type II  |
| reject   | (P\[1-   | Error (  |
| H~O~**   | ɑ\])     | Pr\      |
|          |          | \        |
|          |          | \[ \$    |
|          |          | \b       |
|          |          | eta\$\]) |
+----------+----------+----------+

$\alpha$ sets the limit we are ok with for rejecting H~O~ when it is
true (a *Type 1* error). Alternatively, a *Type II* error is when we do
not reject H~O~ even when it's is false. Importantly,
$\alpha + \beta \neq 1$! Instead, $\alpha + (1-\alpha)$ is the
probability space for H~O~, and $\beta + (1-\beta)$ (or $\beta + power$)
is the probability space for H~A~. How they overlap depends on the
signal, as the the distribution of signals under H~A~ is close to what
we already estimated for confidence intervals! You can visualize the
relationship using the image below. Note the code is hidden given it's
length!

```{r, echo=F}
library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1.5 # sigma H0
m2 <- 3.5 # mu HA
sd2 <- 1.5 # sigma HA
 
z_crit <- qnorm(1-(0.05/2), m1, sd1)
 
# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)
 
# Alpha polygon
y.poly <- pmin(y1,y2)
poly1 <- data.frame(x=x, y=y.poly)
poly1 <- poly1[poly1$x >= z_crit, ] 
poly1<-rbind(poly1, c(z_crit, 0))  # add lower-left corner
 
# Beta polygon
poly2 <- df2
poly2 <- poly2[poly2$x <= z_crit,] 
poly2<-rbind(poly2, c(z_crit, 0))  # add lower-left corner
 
# power polygon; 1-beta
poly3 <- df2
poly3 <- poly3[poly3$x >= z_crit,] 
poly3 <-rbind(poly3, c(z_crit, 0))  # add lower-left corner
 
# combine polygons. 
poly1$id <- 3 # alpha, give it the highest number to make it the top layer
poly2$id <- 2 # beta
poly3$id <- 1 # power; 1 - beta
poly <- rbind(poly1, poly2, poly3)
poly$id <- factor(poly$id,  labels=c("power","beta","alpha"))

ggplot(poly, aes(x,y, fill=id, group=id)) +
  geom_polygon(show.legend = F, alpha=I(8/10)) +
  # add line for treatment group
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show.legend =F) + 
  # add line for treatment group. These lines could be combined into one dataframe.
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  # add vlines for z_crit
  geom_vline(xintercept = z_crit, size=1, linetype="dashed") +
  # change colors 
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  scale_fill_manual("test", values= c("alpha" = "#0d6374","beta" = "#be805e","power"="#7cecee")) +
  # beta arrow
  annotate("segment", x=0.1, y=0.045, xend=1.3, yend=0.01, arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="beta", x=0, y=0.05, parse=T, size=8) +
  # alpha arrow
  annotate("segment", x=4, y=0.043, xend=3.4, yend=0.01, arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="frac(alpha,2)", x=4.2, y=0.05, parse=T, size=8) +
  # power arrow
  annotate("segment", x=6, y=0.2, xend=4.5, yend=0.15, arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="1-beta", x=6.1, y=0.21, parse=T, size=8) +
  # H_0 title
  annotate("text", label="H[0]", x=m1, y=0.28, parse=T, size=8) +
  # H_a title
  annotate("text", label="H[a]", x=m2, y=0.28, parse=T, size=8)
```

The key point is the experimenter sets H~O~ and $\alpha$. Here we
clearly see that in a typical test (like what we illustrated above)
$\alpha$ is split among the top and bottom of the distribution of
signals under H~O~ to create *rejection regions*. Note if we decrease
$\alpha$, which we can, we also decrease the power of the test! On a
related note, we can return to an earlier image

![A 3D visualisaion of PPV, NPV, Sensitivity and Specificity. Luigi
Albert Maria, CC BY-SA 4.0
\<https://creativecommons.org/licenses/by-sa/4.0\>, via Wikimedia
Commons](/images/ppv,%20npv,%20sensitivity,%20and%20specificity.png)

and note that power is equal to sensitivity!

These issues come up in *power analysis*, which is way of using prior
estimates of the distribution of signals to determine appropriate sample
sizes needed to detect significant results. Another form of power
analysis occurs after a test is carried out, but this basically
rehashing the p-value @levine2001 @heckman2022.

This all points to a central idea of NHST. Larger sample sizes let you
pick up smaller differences among groups! We will develop this below and
consider relationships among significance and importance!

## Can we do this without running a sampling experiment every time?

As shown above, we can always use simulations to obtain a p-value.
However, without a computer (and even with) it's cumbersome. We also
have to redo it for every change (for example, what if our sample
contained 19 instead of 18 birds?). Another option is to find an
algorithm that can be used to calculate a distribution that is very
close to what we saw with the simulation.

In the case of our binomial data, very close actually means exact. The
binomial data is an example of data where we can fully describe the
probability outcomes a sample may take. The *binomial distribution*
allows calculations of how often you would expect *s* successes for a
set number of trials (*n-s*) if a population had a proportion of *p* for
the focal trait. This means we use the *binomial distribution* to
calculate our probabilities.

Let's not derive this fully, but just think about it. We have a
proportion of success *p*, so (1 - *p*) is equal to the probability of
failure (since we only have 2 options). For variance, we noted we find
the average squared distance from the focal parameter value (in this
case, a proportion).

For a single draw (what we call *Bernoulli* data), if we assume a
success is equal to 1 and a failure to zero, we could "simply" multiply
the likelihood of each our outcomes by their average squared distance
from the mean

$$ \begin{split} \sigma^2 = (0-p)^2(1-p)+(1-p)^2(p)\\ which \ eventually \ reduces \  to \\ \sigma^2 =p(1-p) \end{split} $$

Since we are assuming each data point is independent ( remember the
multiplication rule?), the probability distribution of getting S
successes from N draws will be

$$ Pr[S] =p^S(1-p)^{N-S} $$

and the variance will be

$$
\sigma_\mu^2 =Np(1-p)
$$

since when you add independent events, you multiply the variances.

Since we don't care about the order of successes and failures in the
sample, we have to think about *combinations* (not developed here), or
how many ways one can arrange *s* successes in *n* draws. Putting it
together, we can write the binomial distribution as

$$
P[n \ successes] = {n\choose s}p^s(1-p)^{n-s}
$$

Using this distribution we can ask for the probability of obtaining any
given number of successes for a given sample size. We can then find how
likely we were to see a signal as more extreme than what we actually
observed in the data by chance if the null hypothesis is true
(p-value!). The *dbinom* function in R uses this distribution.

```{r}
sum(dbinom(0:9,18,.75))+dbinom(18,18,.75)

```

This distributional assumptions also powers the binomial test (also
called the exact binomial test). In R, we can use the binom.test
function to carry it, with the arguments

-   x=number of successes

    -   now you see why we called them successes!

-   n = total number of trials

-   p= expected proportion under the null hypothesis

```{r}
binom.test(x=9, n=18, p=.75)
```

Note for this test the default value for p is .5 (equal chance), so if
you don't enter it that's what will be used.

Notice all our p-values are fairly close. P-values obtained using the
distributional assumptions match exactly., and that obtained by
simulation is very close. It should also be noted that although the
p-value obtained by simulation will vary slightly each time, while those
obtained using the binomial distribution will stay the same.

## Impact of sample size

Now that we can "easily" run a binomial test, let's do it a few times to
see the impact of sample sizes. For example, we could see the same
proportion/signal (50%) of older birds in our sample, but if we only
collected 8 individuals we would not be able to reject H~O~. Note what
happens to our simulation outcomes:

```{r}
number_of_draws <- 8
number_of_simulations <- 1000

sampling_experiment<- data.frame("observed_proportion" = rep(NA, number_of_simulations))
for(i in 1:number_of_simulations){
sampling_experiment$observed_proportion[i] = sum(sample(choices,number_of_draws, replace=T))/number_of_draws
}
sampling_experiment$Proportion = ifelse(sampling_experiment$observed_proportion <= .5, 
                                  '<= .5', ifelse(sampling_experiment$observed_proportion >= 1, '>.5 & < 1', '>= 1'))


ggplot(sampling_experiment,
              aes(x=observed_proportion, fill=Proportion)) +
  geom_histogram(color="black") +
  labs(title="Observed proportions from 1000 random draws",
       x= "Proportion",
       y= "Frequency")
```

So now, we find that outcomes that are as or more extreme than what we
saw in the actual data occur
`r length(sampling_experiment[sampling_experiment$observed_proportion <= .5|sampling_experiment$observed_proportion >= 1,])`
times. So, in taking 1000 random draws from our null population, we only
saw what we actually observed (or something more extreme)
`r length(sampling_experiment[sampling_experiment$observed_proportion <= .5|sampling_experiment$observed_proportion >= 1,])*.001`%
of the time. Similarly,

```{r}
binom.test(4,8,p=.75)
```

leads to a p-value which is \>.05, so we fail to reject H~O~. Again,
this relates to how sampling error interacts with sample size, much as
we saw when constructing confidence intervals. This means we have to
differentiate between *statistical significance* and *importance*.

Given a large enough sample size, we can detect very small differences
from our parameter value under the null hypothesis. For example, what if
data from another population of finches showed 780 older birds out of a
sample of 1000 birds that collided with windows. If we assume the
population distribution in regards to age is the same, we are still
testing

$$
\begin{split}
H_O: p=.75 \\  
H_A: p \neq .75
\end{split}
$$

In our sample, we found a signal of `r 780/1000`, which is very close to
.75. However, we find a p-value of

```{r}
binom.test(780,1000, p = .75)
```

Is the slight increase in older birds really important to understanding
the population? Maybe, or maybe not. The point is we have to understand
the difference between estimates and significance and the more nebulous
idea of *importance*.

## Estimates and p-values work together

This is one way estimates and NHST work together. Estimate focuses on
the sample (Given sampling error, where do we think true parameter
lies?). Hypothesis testing focuses on the likelihood of the signal given
the null distribution (how likely were we to observe data that we did, a
la the p-value), but gives no information about the actual difference
(which could be important for determining if something really matters!).

## One-sided tests

In introducing the p-value (and estimation) we focused on two-sided (or
two-tailed) tests. This means we considered deviations from our value
under the null hypothesis (for p-values) or via sampling error (for
confidence intervals) based on their magnitude, and not direction.
However, we can instead decide we want to consider differences to one
"side" of our value of interest. Following this idea, we have 3 options
for our null and alternative hypotheses (note *C* is a constant value
here!):

+-----+-----+-----+-----+
|     | -   | -   | -   |
|     |   \ |  \* |  \* |
|     |     |     |     |
|     | \*  |     |     |
|     | T w |   F |   F |
|     | o - |     |     |
|     | s   |     |     |
|     | i   |   o |   o |
|     | ded |     |     |
|     | (   |     |     |
|     |     |   c |   c |
|     | t y |     |     |
|     | p i |   u |   u |
|     | c a | sed | sed |
|     | l ) |     |     |
|     | \   |     |     |
|     | *\* |  on |  on |
|     |     |     |     |
|     |     | s i | s i |
|     |     | g   | g   |
|     |     | n   | n   |
|     |     | als | als |
|     |     |     |     |
|     |     | g r | l   |
|     |     | e   | ess |
|     |     | a   |     |
|     |     | ter | t   |
|     |     |     | han |
|     |     | t   | p r |
|     |     | han | e d |
|     |     | p r | i   |
|     |     | e d | c   |
|     |     | i   | ted |
|     |     | c   | in  |
|     |     | ted | n u |
|     |     | in  | l l |
|     |     | n u | \   |
|     |     | l l | *\* |
|     |     | \   |     |
|     |     | *\* |     |
+-----+-----+-----+-----+
| -   | p = | p   | p   |
|   \ | C   | \<= | \>= |
|     |     | C   | C   |
|  \* |     |     |     |
| H\~ |     |     |     |
|     |     |     |     |
|     |     |     |     |
|  O\ |     |     |     |
|     |     |     |     |
|     |     |     |     |
|  \~ |     |     |     |
|     |     |     |     |
|   \ |     |     |     |
| *\* |     |     |     |
+-----+-----+-----+-----+
| -   | p   | p   | p   |
|   \ | \$  | \>  | \<  |
|     | \n  | C   | C   |
|  \* | e   |     |     |
| H\~ | q\$ |     |     |
|     | C   |     |     |
|     |     |     |     |
|  A\ |     |     |     |
|     |     |     |     |
|     |     |     |     |
|  \~ |     |     |     |
|     |     |     |     |
|   \ |     |     |     |
| *\* |     |     |     |
+-----+-----+-----+-----+

For example, Claramunt et al @claramunt2022 wished to consider if roads
impaired bird movement. To do they considered if banded birds were more
likely to be recapture in one of 3 areas across a road from their
original location or one of 6 on the same side on which they were
captured. They were only interested if roads reduced bird movement, so
they were justified in using a *sided* test. These tests move all the
rejection region to one side. You can run these by adding an
*alternative* argument to binom.test

```{r}
binom.test(116,641, p=.33, alternative = "less")
```

Here we reject H~O~, where

$$
\begin{split}
H_O: p>=.33 \\ 
H_A: p < .33
\end{split}
$$

However, sided or tailed tests should be rarely used? Why? Because it
can be too tempting to use a sided test after observing the data! A
signal that is not significant at the $\alpha$ =.05 level using
two-sided tests can be significant as a one-tailed test.

If you do use these, note they correspond to confidence bounds instead
of intervals. Again, the full rejection region is placed on one side of
the estimate.

## Tying it all together

Let's return to our bird collision example and connect estimation and
p-values (and teach you how to estimate confidence intervals for
binomial data).

Remember, we found 9 older birds in our sample of 18. This means our
estimate for the proportion of older birds `r 9/18`. Just like for
continuous data, we can consider sampling error in our estimate. Let's
think about how that might happen.

<details>

<summary>

In short, the standard error of *p* is

$$
SE(p) = \sqrt{\frac{p(1-p)}{N}}
$$but since we don't know p, we use our estimate

$$
SE(\hat{p}) = \sqrt{\frac{(\hat{p}(1-\hat{p})}{N}}
$$

To find out a little more, click here.

</summary>

For a single draw (Bernoulli data), if we assume a success is equal to 1
and a failure to zero, we could "simply" multiply the likelihood of each
our outcomes by their average squared distance from the mean

$$
\begin{split}
\sigma^2 = (0-p)^2(1-p)+(1-p)^2(p)\\
which \ eventually \ reduces \  to \\
\sigma^2 =p(1-p)
\end{split}
$$

If we move to N independent draws, we predict the average observed
outcome (or the mean number of successes!) will be

$$
\mu_S = Np
$$

Since we are assuming each data point is independent, the variance of N
draws will be

$$
\sigma_\mu^2 =Np(1-p)
$$

so

$$
\sigma_\mu =\sqrt{Np(1-p)}
$$

Notice in doing this we went from a proportion to a number of successes!
Now we can use our typical equation for standard error of the means

$$
[\sigma_{\mu_s} = \frac{\sigma}{\sqrt{N}} = \frac{\sqrt{(Np(1-p)}}{\sqrt{N}}  = \sqrt{\frac{(p(1-p)}{N}} \ ] \sim [s_{\overline{Y}} = \frac{s}{\sqrt{N}} =   \frac{\sqrt{(N\hat{p}(1-\hat{p})}}{\sqrt{N}} = \sqrt{\frac{(\hat{p}(1-\hat{p})}{N}}]
$$

This is actually a bit tricky as it assumes some connections between
categorical and continuous data, and many ways have been proposed to do
this [@subedi2019], but this gets us close.

</details>

It turns out our estimate of the standard error may be biased,
especially for small sample sizes or extreme (close to 0 or 1) values of
*p*. For that reason, several ways have been suggested to calculate
confidence intervals [@subedi2019] . Note, for example, the
*binom.confint* function in the *binom* package gives multiple outcomes.
For the function,

-   the first argument is the number of successes

-   the second argument is the number of trials

```{r}
library(binom)
binom.confint(9,18)
```

For now, we will use method labelled the Agresti-Coull method, which
adjusts for slight bias in other estimates and is useful across sample
sizes.

```{r}
using_distribution <- dbinom(0:18,18,.75)
finches <- data.frame (Number = 0:18, Probability = using_distribution)
finches$Proportion <- finches$Number/18
finches$criteria <- "retain"
finches$criteria[pbinom(finches$Number, 18, .75) < .025] <- "reject"
finches$criteria[(1-pbinom(finches$Number, 18, .75)) < .025] <- "reject"
proportion_observed = data.frame(Proportion = 9/18, Probability = .15)#sets height
ggplot(finches, aes(x = Proportion, y = Probability)) + 
  geom_bar(stat="identity", aes(fill = criteria)) + 
  geom_segment(x = .29031, xend = .70968,y= .15 , yend =.15) +
  geom_vline(xintercept = .75, color = "blue") + geom_vline(xintercept = 9/18, color = "black") +
  geom_point(data= proportion_observed) +
  ggtitle("Comparing p-values and confidence intervals for finch problem")
```

Note we see our rejection region in red; it also contains our estimate!
Similarly, the 95% confidence interval for our estimate does not contain
the paramater value under the null hypothesis!

## Next steps

Make sure you understand the above concepts (i.e., how p-values are is
related to null hypotheses and how to interpret them!). Our following
chapters will extend this idea to different types of data, starting with
continuous data from a single sample in the next chapter.
